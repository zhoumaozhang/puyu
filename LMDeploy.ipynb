{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMDeploy\n",
    "## 大模型部署背景\n",
    "部署：将训练好的模型放在特定的环境中运行\n",
    "服务器端有GPU/TPU/NPU。\n",
    "内存开销大：20B模型加载参数需要40G，175B模型需要350G。推理过程中模型会对K和V进行缓存。\n",
    "访存瓶颈：算力受限于显存带宽。\n",
    "动态请求：请求量、请求时间、token生成数量不确定\n",
    "## 大模型部署方法\n",
    "### 模型剪枝\n",
    "### 知识蒸馏\n",
    "### 量化\n",
    "FP32一个数占用4个字节。\n",
    "量化降低了访存量。\n",
    "## LMDeploy简介\n",
    "模型高效推理，模型量化压缩，服务化部署\n",
    "## 动手实践\n",
    "### 与模型进行对话\n",
    "![image](对话1.png \"image\")\n",
    "![image](对话2.png \"image\")\n",
    "### LMDeploy服务\n",
    "模型推理、API接口、客户"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
